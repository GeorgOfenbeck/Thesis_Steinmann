\documentclass[a4paper,12pt]{article}
\usepackage[top=2cm]{geometry}
\usepackage{color}
\usepackage{listings}
\usepackage{calc}
\usepackage{booktabs}
\usepackage{subfig}

\usepackage{makeidx}

\usepackage[colorlinks,bookmarks,pagebackref]{hyperref}
\usepackage{graphicx}
\newlength{\imgwidth}
\newlength{\imgleftoverhang}
\newcommand{\umlDiagram}[1]{%
	\settowidth{\imgwidth}{\includegraphics{out/diagrams/#1.pdf}}%
	\setlength{\imgwidth}{\minof{0.5\imgwidth}{\textwidth}}%
	\par\vskip0.5cm\noindent\makebox[\textwidth][c]{%
	\includegraphics[width=\imgwidth]{out/diagrams/#1.pdf}%
}\vskip0.5cm}

\newcommand{\umlFloat}[2]{
\begin{figure}[htbp]
\umlDiagram{#1}
\caption{#2}
\label{#1}
\end{figure}
}

\newcommand{\graphNoVskip}[1]
{
	\settowidth{\imgwidth}{\includegraphics{graphs/#1.pdf}}
	\setlength{\imgleftoverhang}{0.1\textwidth}
	\setlength{\imgwidth}{1.205\textwidth}
	\hfuzz50pt
	\noindent\hskip-\imgleftoverhang
	\hskip-0.25in\includegraphics[width=\imgwidth]{graphs/#1.pdf}
	\hfuzz0.1pt
}
	
\newcommand{\graph}[1]
{
	\par\vskip0.5cm
	\graphNoVskip{#1}
	\vskip0.5cm
}

\newcommand{\graphFloat}[2]
{
\begin{figure}[htbp]
\graphNoVskip{#1}
\vskip-0.5cm
\caption{#2}
\label{#1}
\end{figure}
}

\newcommand{\graphFloatTwo}[5]
{
\begin{figure}[htbp]
\subfloat[#3]{\graphNoVskip{#2}}\\
\subfloat[#5]{\graphNoVskip{#4}}
\caption{#1}
\label{#2}
\end{figure}
}

\newcommand{\umlFloatCap}[3]{
\begin{figure}[tbh]
\umlDiagram{#1}
\caption[#2]{#3}
\label{#1}
\end{figure}
}
\newcommand{\umlRef}[1]{\autoref{#1}}

\newcommand{\class}[1]{\textbf{#1}}
\newcommand{\method}[1]{\textsf{#1}}

\lstset{
  showstringspaces=false,
  breaklines=true,
  tabsize=4,
  numbers=left, stepnumber=2, numberstyle=\tiny, numbersep=10pt
}

\lstdefinestyle{pseudoCode}
{%%stringstyle=\textit
}

\sloppy
\renewcommand{\floatpagefraction}{0.7}
\renewcommand{\dblfloatpagefraction}{0.7}
    
\title{Applying the Roofline Model}
\author{Ruedi Steinmann}
\date{\today}

\begin{document}
\maketitle

\begin{abstract}
This is the paper's abstract \ldots
\end{abstract}

\tableofcontents

\section{Introduction}
'The free lunch is over' \cite{FreeLunchIsOver} In the past decades, the CPU
manufactures were reliably able to increase the clock speed. However, this has
become harder and harder due to not just one but several physical issues. To
increase performance despite the limited clock speed, the CPUs became more
complex.

From the perspective of software performance optimization, the increased
complexity made performance observations harder to understand. Since the trend
to more complex CPUs is likely to continue in the future, an insightfule and
easy-to-understand performance model would be of high value.
 
The roofline model \cite{Roofline} premises to fulfill these criteria. The key
observation which led to this model is that, for the forseable future, the
transfer bandwidth between CPU and Memory will often be the limiting factor of
system performance. 

If an algorithm manages to largely operate on data which can be held in the CPU
caches, off-chip bandwith will not be an issue, in contrast to an algorithm 
excessively accessing data directly from memory. The balance betweeen memory 
transfer and floating point computation is described by the 'operational
intensity', measuring the number of performed operations per transferred byte.

\graphFloat{yonah/overview}{Yonah: Overview of different Kernels}
The roofline model combines the operational intensity with the performance in a
2D graph (fig. \ref{yonah/overview}). Each measurement results in a point on the
plot. Multiple points are connected with a line to a series. The graph is on a
log-log scale.

The peak floating point performance appears as a horizontal line. There can be
different bounds depending on the technology or the number of cores used. The
memory bandwith forms a line of unit slope. There can again be multiple bounds,
depending on the number of cores or the access pattern (random vs. linear).

The horizontal and diagonal lines give the roofline model it's name. The point
where the lines intersect is called the ridge point. 

A roofline graph contains useful information for performance optimization. If
the operational intensity of a kernel lies to the left of the ridge point, the
kernel is memory bound. Peak floating point performance can only be reached by
increasing the operational intensity. If the kernel lies to the right of the
ridge point, memory bandwith is no limiting factor. If the kernel does not reach
the peak performance, there must be other performance limiting factors.

When comparing the roofline graphs of different machines, the position of the
ridge point is an important clue of how difficult it is to reach a good
performance. The further the ridge point lies to the right, the higher is the
operational intensity required to achieve peak performance. 

\subsection{Goals for the Master Thesis}
Since we did not find a tool capable of easily creating roofline plots, we
decided to create one our selves during this thesis. It should be able to
measure the performance and operational intensity of a program, and to compose
the results of different measurements into one roofline plot.
  
Code to be measured can come either in the form of a relatively isolated
routine, typically containing a single kernel, or in the form of a larger
program, containing multiple different kernels. While routines are treated as
atomic units, it is desirable to split the larger program into it's different
algorithms and do the analysis for each algorithm separately. In the rest of
this paper, we will use the term kernel for a routine or part of a program which
is analyzed on it's own.

To obtain the peak performance lines for the roofline model, we need to run
micro benchmarks. To get the data points for a kernel, we need to measure the
performance and the operational intensity. 

Performance is defined as amount of work per unit of time. The amount of work
and how it is defined is determined by the actual kernel and may not be need to
be measured directly by the measurement tool. The time required to do the work
has to be measured.

Operational intensity is defined as operations per byte of data traffic. Since
the operational intensity cannot be measured directly, we have to measure the
operation count and the data traffic. Depending on the problem, different
definitions of operation and data traffic make sense.

An operation could be a floating point operation, resulting int the well known
"flops" unit, either single or double precision. But an operation could as well
be defined as machine instruction, integer operation or others.

The point where data traffic is measured is typically between the last level
cache of the processor and the DRAM, but it could be measured as well between
the different cache levels or between L1 cache and processor.

The micro benchmarks are typically designed to either transfer as much data per
unit of time or to execute as may operations per unit of time. Thus the
measurement capabilities needed to evaluate the actual problems can be reused
for the benchmarks. 

In summary, we need to measure the following quantities:
\begin{itemize}
\item execution time
\item memory traffic
\item operation count
\end{itemize}

\subsection{Performance Counters}
Recent x86 CPUs contain typically two performance counters. Each can be
configured to measure one performance event. There are many events, 119 on a
CoreDuo. Examples are:
\begin{description}
  \item[INSTRUCTION RETIRED] occurs for every retired instruction
  \item[DBUS BUSY] occurs for every cycle during which data bus is busy
  \item[SSE PRE MISS] occurs when an SSE instruction misses all cache levels
  \item[BR TAKEN RET] occurs for every retired taken branch instruction 
\end{description}

When working with the linux operation system, the performance counters are
configured and read using a kernel API. The kernel takes care of isolating
different processes, such that each process can configure and use the counters
according to the specific requirements, without interfering with other processes.

Using the perfomance counters, it is possible to measure execution time, memory
traffic and operation count. For the execution time either the cycle counter or
the system timer can be used. The memory traffic can be determined using the
performance counters for counting cache misses and write back operations. An
other option is to use the counters for bus transfers. Measuring the operation count
depends heavily on the definition of operation, but is typically measurable with
the performance counters, too.

\section{Measurement Setup}
All measurements are performed on an IBM X60 Thinkpad, featuring an Intel
CoreDuo CPU (Family 6, Model 14). The CPU is based on the Yonah
microarchitecture, which similar to the Pentium-M. The CPU contains two cores,
each having a 32KB instruction and a 32KB data L1 cache, 8 ways set associative,
with 64 bytes line size. The two cores share a 2MB unified L2 cache, again 8
ways set associative and with 64 bytes line size. The core frequency can scale
between 1GHz and 1.83GHz. The bus frequency is 167MHz. 

The main memory consits of two 2GB DDR2 modules, totaling in 4GB available
memory. The theoretical throughput of the memory is 5.12 GB/s, which is 2.80
Bytes per core cycle, if the CPU runs at 1.83GHz.

We used XUbuntu 11.10, running a Linux 3.0.0-16 kernel in 32 bit mode, since
the CPU does not support the 64 bit mode.

The performance counters are instrumented using the 'perf event' kernel
interface \cite{unoffPerfEventsWebPage}, using libpfm4  \cite{libpfm4Docu} to
generate the required parameters.

We used 'coreduo::UNHALTED\_CORE\_CYCLES' for measuring time. For the operation
count, we used the follwing definitions for operation: 
\begin{description}
\item[SinglePrecisionFlop] SSE single precision operations.
\\{\footnotesize
'coreduo::SSE\_COMP\_INSTRUCTIONS\_RETIRED:SCALAR\_SINGLE'\\
+2*'coreduo::SSE\_COMP\_INSTRUCTIONS\_RETIRED:PACKED\_SINGLE'}
\item[DoublePrecisionFlop] SSE double precision operations.
\\{\footnotesize
'coreduo::SSE\_COMP\_INSTRUCTIONS\_RETIRED:SCALAR\_DOUBLE'\\
+2*'coreduo::SSE\_COMP\_INSTRUCTIONS\_RETIRED:PACKED\_DOUBLE'}
\item[CompInstr] Computational instructions retired. Counts SSE instructions
and x87 instructions. Used for x87 code. 
\\{\footnotesize 'coreduo::FP\_COMP\_INSTR\_RET'}
\item[SSEFlop] SSE operations, sum of SinglePrecisionFlop and
DoublePrecisionFlop
\end{description}

We used two variants of measuring the memory transfer volume. The {\bf MemBus}
variant uses 64*'coreduo::BUS\_TRANS\_MEM', which measures the transfers on the
system bus. The {\bf MemL2} variant uses the counters for the L2 cache line
allocation and eviction, namely
64*('coreduo::L2\_LINES\_IN:SELF'+'coreduo::L2\_M\_LINES\_OUT:SELF'), combined
with 8*'coreduo::SSE\_NTSTORES\_RET' to take non temporal stores into account.

\subsection{Cache}
\graphFloat{yonah/FFTwarm}{Yonah: Influence of the initial cache state}
\graphFloat{yonah/FFTwarmPerformance}{Yonah: Influence of the initial cache
state on the Performance} 

Specially for short running kernels, the initial state of the cache can have a
big impact on the memory traffic and execution time.  The impact can be
controlled by making sure the caches are either warm or cold.

In our tool, it is possible to specify the desired inital state of the code and
the data separately. We used the fast fourier transformation implementation of
the Intel Math Kernel Libraries as showcase. Figures \ref{yonah/FFTwarm} and
\ref{yonah/FFTwarmPerformance} show the results when the cache is cold
(FFT-Mkl), only the data is warmed up (FFT-Mkl-Data), only the code is warmed up
(FFT-Mkl-Code) or both data and code is warmed up (FFT-Mkl-Data-Code).

The following snippet shows the relevant logic:

\begin{minipage}{\textwidth-18pt}
\begin{lstlisting}[language=C++]
// should the code cache be warm?
if (getWarmCode()) {
	// Tell the kernel to warm the code cache,  
	// which usually results in the kernel 
	// beeing executed once.
	getKernel()->warmCodeCache(); 
	
	// Should we clear the data?
	if (!getWarmData()) {
		getKernel()->flushBuffers();
	}
} else { // Code cache should be cold.
	// Access a large memory buffer and execute  
	// a lot of code, this clears the code cache.
	clearCaches();
	
	// Should the data be warm?
	if (getWarmData()) {
		// Warm the data cache by accessing each 
		// cache line of the data buffer(s).
		getKernel()->warmDataCache();
	} else {
		// The data cache should be cold. 
		getKernel()->flushBuffers();
	}
} 
\end{lstlisting}
\end{minipage}

Getting cold caches can be quite tricky. See
\cite{Whaley:2008:AAC:1462062.1462065} section '3.
CACHE FLUSHING METHODS WHEN TIMING ONE INVOCATION' for details.

To clear the code cache, we use the traditiona method of accessing a large
memory buffer and executing more than 32KB code to flush the L1 code cache.

In addition, the kernels report the buffers they allocated. This is used to
flush these buffers with the 'clflush' instruction, not affecting code which is
already in the cache. This is implemented by the \method{flushBuffers()}
method in the above code snippet.

\subsection{Frequency Scaling}
The core frequency is not constant in current processors. Since the memory
latencies and throughputs do not scale with the core frequency, a lower core
frequency will generally cause a higher percentage of the peak performance to be
reached by the kernel.

On Linux, frequency scaling is controlled so called governors. We used the
'performance' governor for all our measurements, which fixes the core frequency
to the maximum.

\section{Single Threaded Accuracy and Precision}
First, let's recapitulate the definitions of accuracy and precision.
\cite{accuracyAndPrecision} Accuracy of a measurement system is the closeness of
the measurement results to the true value.
Precision is the degree to which repeated measurements show the same result.

A standard technique to increase precision is to repeat a measurement and use
a measure of tendency. This generally increases the precision of the result, but
does not improve the accuracy in presence of systematic errors.

To analyze the results of single threaded measurements we designed three groups
of measurements. Each group is focused on one specific quantity, namely time,
transfer volume and operation count. We measured on an idle system. All
measurements are executed with cold caches.

In all groups we used the following kernels:
\begin{description}
\item[ADD] Repeat adding a constant to an accumulator. Everything is performed
in registers. We use multiple accumulators and unroll the loop. 
\item[Read] Read a buffer from memory
\item[Write] Overwrite a buffer in memory
\item[WriteStream] Overwrite a buffer in memory, using non temporal store
instructions
\item[Triad] Perform $a_i=b_i+k*c_i$. This involves reading $a$, $b$ and $c$ and
writing $a$ back.
\end{description}

\subsection{Transfer Volume}

\graphFloatTwo{Transfer Volume}
{yonah/valTBValues}{MemBus}
{yonah/valTBALTValues}{MemL2}

\graphFloatTwo{Error of Transferred Bytes}
{yonah/valTBError}{MemBus}
{yonah/valTBALTError}{MemL2}

We tried to estimate the amount of memory that has to be transferred. For pure
reading, we expect to observe the whole buffer beeing transferred to the core.
When writing is involved, we expect some of the writes to be held back in the
cache. For the write kernel we presume that the whole cache is used for write
back caching, and one third of the cache for the triad kernel, since there are
three buffers. The streaming write kernel should write the whole buffer once.

Figure \ref{yonah/valTBValues} shows that we roughly observe the expected amount
of memory transfer. Around an expected transfer volume of 2MB we overestimate
the influence of the write back cache. The CPU seems to write data back to the
memory before the cache is completely filled. The MemBus and MemL2 variants
produce similar result. Note that the MemL2 variant measures less than the
expected memory tansfer for large buffer sizes.

The errors (see fig. \ref{yonah/valTBError}) of the MemL2 variant are
smaller than those of the MemBus variant.

\graphFloat{yonah/valTBALTFlushValues}{MemL2: Memory Transfer during Cache
Flush after kernel execution}

To validate our assumption on the write back cache, we measured the memory
transfer while flushing the cache using 'CLFLUSH' after the execution of the
kernel completes. (fig. \ref{yonah/valTBALTFlushValues}) There is a small
overhead for the Read kernel. The data of the write kernel shows that almost the
whole 2MB 2nd level cache is used for write back caching. For the Triad kernel
one could suspect that each buffer receives about one third of the cache, which
is affirmed by the data.

When measuring on an Intel Core, we observed a strange behaviour. While the Read
and the Triad kernels behaved as expected, the we observed a memory transfer of
half the buffer size for the write kernel, independent of the buffer size. We
used the SSE intrinsics. Switching to the normal integer registers produced the
expected results.

\graphFloatTwo{Transferred Bytes of the Arithmetic Kernels}
{yonah/valTBArithTBValues}{MemBus}
{yonah/valTBALTArithTBValues}{MemL2}

When measuring the memory transfer of the arithmetic kernels, we observe an
overhead of around 2.5KB for for short running kernels. When the execution time
exceeds 1ms to 10ms, the memory transfer starts to correlate with the execution
time. There is a huge difference between the BusMem and BusL2 variants. The
BusMem variant measures an overhed of around one byte every 20 cycles, while the
BusL2 variant measures one byte every 300-1600 cycles (see fig.
\ref{yonah/valTBArithTBValues} and \ref{yonah/valTimeArithValues}) This
difference is plausible since the BusMem variant is likely to pick up any noise
on the memory bus.

\subsection{Time}
\graphFloat{yonah/valTimeArithValues}{Execution Time of the ADD kernel}
\graphFloat{yonah/valTimeArithError}{Error of the Execution Time of the
ADD kernel}

For the ADD kernels, figure \ref{yonah/valTimeArithValues} shows that the
execution time is around one cycle per operation and does not differ between SSE
and x87, which is consistent with the information found in the Intel manual.
The same applies for the MUL kernels, with a throughput of around $0.5$
multiplications per cycle. The error is small for operation counts above 10000
(fig \ref{yonah/valTimeArithError}).

\graphFloat{yonah/valTimeMemValues}{Execution Time of the memory kernels} 
\graphFloat{yonah/valTimeMemError}{Error of the Execution Time of the memory kernels}
For the kernels causing memory transfer, we observe a strong correlation between
expected memory transfer and execution time (fig. \ref{yonah/valTimeMemValues}),
but the errors far exceed those observed for the arithmetic kernels (fig.
\ref{yonah/valTimeMemError}).


\subsection{Operation Count}
\graphFloat{yonah/valOpValues}{Operation Count}
Figure \ref{yonah/valOpValues} how that we get
almost perfect results for measuring the operation count. The error for the x87
ADD and MUL could be explained by an overhead of around 50 respectively
500 floating point instructions.

\clearpage
\subsection{Handling the Variance}
\label{sec:SingleThreadedHandlingTheVariance}
In the previous sections we saw that our measurements produce plausible results.
But as soon as the kernels cause memory transfer, we observe a high variance. We
presume this is due to various activities happening in the background during the
measurement. (Task switches etc).

These activities cause events to be measured which are not related to
the execution of the kernel. Fortunately, additional events only increase the
measured execution time, memory transfer or operation count. Ideally, at least
one execution does not include any overhead. In this case the true value is the
minimum value observed.

Based othese considerations, we repeat each measurement $K$ times and
discard all but the minimum value.

\graphFloatTwo{Error of the Transfer Volume with $K=10$}
{yonah/valTBMinError}{MemBus}
{yonah/valTBALTMinError}{MemL2} 

Using this scheme with $K=10$ for measuring the transfer volumes, the samples
fall in a range of about 5\%. The bias is under 10\% except for transfer volumes
under 20KB and with a peak of up to 40\% around 2MB for the kernels involving
writes. The differece in precision between MemBus and MemL2 disappeared
almost completely. For large transfer volumes, the accuracy of the MemBus
variant is superior. (fig. \ref{yonah/valTBMinError})

\graphFloat{yonah/valTimeMemMinError}{Yonah: Error of the Execution Time with
$K=10$} 

Applying the same scheme to measuring the execution time of the memory kernels
yields a precision of about 10\%. Since we cannot derive an expected value, we
chose the minimum observed value as reference.
(fig. \ref{yonah/valTimeMemMinError})

\graphFloat{yonah/valTimeArithMinError}{Yonah: Error of Execution Time with
$K=10$}

\graphFloat{yonah/valOpMinError}{Yonah: Error of Operation Count with
$K=10$}

Finally, figures \ref{yonah/valTimeArithMinError} and \ref{yonah/valOpMinError}
show that the arithmetic kernels do not pose any problems.

Summarizing our results, using the minimum of $K$ scheme with $K=10$ the
arithmetic kernels can be measured with high precision and accuracy. The results
of measuring execution time and memory transfer of memory intensive kernels are
scattered within 10\% of the reference value. The memory tranfer volume
is overestimated by about 10\% for transfer volumes under 10MB

\clearpage
\section{Multi Threaded Accuracy and Precision}
For the validation of multithreaded measurements, we use the same measurements
as for the single threaded validation, but run a separate workload instance on
each core. Using a barrier, we make sure the two kernels start at the same time. 

Since the Yonah does not utilize HyperThreading, we don't expect
an arithmetic kernel running on one core affecting a memory intensive kernel on
the other core. Therefore the workloads run in parallel are not mixed.


\subsection{Transferred Volume}
\graphFloatTwo{Transfer Volume of the Read Kernel}
{yonah/valTBThReadValues}{MemBus}
{yonah/valTBALTThReadValues}{MemL2} 

\graphFloatTwo{Transfer Volume of the Write Kernel}
{yonah/valTBThWriteValues}{MemBus}
{yonah/valTBALTThWriteValues}{MemL2} 

\graphFloatTwo{Transfer Volume of the Triad Kernel}
{yonah/valTBThTriadValues}{MemBus}
{yonah/valTBALTThTriadValues}{MemL2} 

\graphFloat{yonah/valTBThReadPoint}{Distribution of the Results of the Read
Kernel using MemBus} 

\graphFloat{yonah/valTBThWritePoint}{Distribution of the Results of the Write
Kernel using MemBus}

\graphFloat{yonah/valTBThTriadPoint}{Distribution of the Results of the Triad
Kernel using MemBus} 

The MemBus variant measures all transfers on the bus. Therefore, the
measurer of each workload should see the traffic of both kernels. The MemL2 variant
should separate the traffic of the two cores. On figures
\ref{yonah/valTBThReadValues},
\ref{yonah/valTBThWriteValues} and
\ref{yonah/valTBThTriadValues} we see the expected results for the MemL2
variant. The precision is comparable to the single threaded measurements, the
overhead is slightly higher.

In contrast, the MemBus variant shows a huge variation. Choosing 4MB as
representive buffer size, figures \ref{yonah/valTBThReadPoint},
\ref{yonah/valTBThWritePoint} and \ref{yonah/valTBThTriadPoint} show the
distribution of the results. There is a cluster around the expected result of
twice the expected transfer volume (each workload seeing the transfer of both
cores), but also a cluster around once the expected transfer volume. It seems
that in about half of the measurement runs, the kernel only see their own
traffic. 

\subsection{Time}
\graphFloat{yonah/valTimeThAddArithValues}{Execution Time of two ADD Kernels
running at the same time} As expected, running two arithmetic kernels in
parallel does not affect the measured execution time (fig. \ref{yonah/valTimeThAddArithValues}).

\graphFloat{yonah/valTimeThReadMemValues}{Execution Time of two Read Kernels
running at the same time}

\graphFloat{yonah/valTimeThWriteMemValues}{Execution Time of two Write Kernels
running at the same time}

\graphFloat{yonah/valTimeThTriadMemValues}{Execution Time of two Triad Kernels
running at the same time}

For the memory kernels we don't see much difference in execution time between
the single threaded and the multithreaded results for expected transfer volumes
below 1MB. We expected to observe a larger difference. For larger buffers the
expected difference of about a factor of two can be observed, although outliers
are frequent. (fig. \ref{yonah/valTimeThReadMemValues} and
\ref{yonah/valTimeThWriteMemValues})

For the triad kernel there is almost no
difference. (fig. \ref{yonah/valTimeThTriadMemValues})

\subsection{Operation Count}

\graphFloat{yonah/valOpThValues}{Operation Count}

As for the single threaded case, measuring the operation count does not pose any
problems. (fig. \ref{yonah/valOpThValues})

\clearpage
\subsection{Handling the Variance}
We again use the minimum of $K=10$ scheme. (see
\ref{sec:SingleThreadedHandlingTheVariance})

\graphFloatTwo{Error of the Transfer Volume of the Read Kernel with $K=10$}
{yonah/valTBThReadMinError}{MemBus}
{yonah/valTBALTThReadMinError}{MemL2} 

\graphFloatTwo{Error of the Transfer Volume of the Write Kernel with $K=10$}
{yonah/valTBThWriteMinError}{MemBus}
{yonah/valTBALTThWriteMinError}{MemL2} 

\graphFloatTwo{Error of the Transfer Volume of the Triad Kernel with $K=10$}
{yonah/valTBThTriadMinError}{MemBus}
{yonah/valTBALTThTriadMinError}{MemL2} 

When measuring the transfer volume we achieve a variation below 10\%. The
overhead is around 20\% for small buffers, and goes towards zero for transfer
volumes exceeding 10MB. (figs \ref{yonah/valTBThReadMinError},
\ref{yonah/valTBThWriteMinError} and \ref{yonah/valTBThTriadMinError})

\graphFloat{yonah/valTimeThReadMemMinError}{Error of the Execution Time of two
Read Kernels running at the same time with $K=10$}

\graphFloat{yonah/valTimeThWriteMemMinError}{Error of the Execution Time of two
Write Kernels running at the same time with $K=10$}

\graphFloat{yonah/valTimeThTriadMemMinError}{Error of the Execution Time of two
Triad Kernels running at the same time with $K=10$}

For the execution time, we observe huge variations for small buffer sizes. If
the expected transfer volume exceeds 20KB, the variations drop to around 10\%
and become large again for transfer volumes exceeding 10MB. (figs.
\ref{yonah/valTimeThReadMemMinError}, \ref{yonah/valTimeThWriteMemMinError} and
\ref{yonah/valTimeThTriadMemMinError})

\graphFloat{yonah/valTimeThAddArithMinError}{Error of the Execution Time of two
ADD Kernels running at the same time with $K=10$}

The variations observed when running two ADD kernels in parallel are small
except for iteration counts below 100000. (fig.
\ref{yonah/valTimeThAddArithMinError})

\graphFloat{yonah/valOpThMinError}{Error of the Operation Count with $K=10$}
As expected, the results for the operation count are perfect (fig. \ref{yonah/valOpThMinError})

Summarizing our results, using the minimum of $K$ scheme with $K=10$ the
arithmetic kernels can be measured with high precision and accuracy. 

The results of measuring the transfer volume of memory intensive kernels are
scattered within 10\% of the reference value and overestimated by about 20\% for
transfer volumes below 10MB. The results of measuring the execution time have
generally a high variance, except for transfer volumes between 20KB and 10MB.

\clearpage

\section{Experimental Results}
We used our tool to measure various kernels. For each kernel we describe the
kernel and the measurement setup and show the resulting roofline plot.

\subsection{BLAS}
Basic Linear Algebra Subprograms (BLAS) is a standard API for basic linear
algebra operations. The functionality is divided into three levels:

\begin{description}
\item[Level 1] Vector-Vector Operations 
\item[Level 2] Matrix-Vector Operations 
\item[Level 3] Matrix-Matrix Operations 
\end{description}

For each level, we chose a representative operation and generated roofline plots
using the Intel Math Kernel Library (MKL) \cite{MKL} and the OpenBlas
\cite{OpenBlas} implementations.

\graphFloat{yonah/daxpy}{Roofline Plot of the Vector-Vector Multiplication
(MemL2, DoublePrecisionFlop)}
For level 1, we chose the 'daxpy' operation, defined as $\bf y \leftarrow \alpha
\bf x + \bf y$ with double vectors. There is no reuse in this kernel. For every
vector element triple, two operations are performed. Since the vector sizes
we measure fits into the last level cache, we don't have to take the write
backs into account. The maximal operational intensity is $2/(2*8)=1/8=0.125$
which is confirmed by figure \ref{yonah/daxpy}.

\graphFloat{yonah/dgemv}{Roofline Plot of the Matrix-Vector Multiplication
(MemL2, DoublePrecisionFlop)} 
For level 2, we chose the 'dgemv' operation,
defined as $\mathbf y \leftarrow \alpha A \mathbf x  + \beta \bf{y}$ with double
vectors and matrices. For an $n\times n$ matrix and vectors of size $n$, the
operation count is $2n$ for the vector scaling and $2n^2$ for the multiplication of the matrix with the vecor and the
vector addition, totaling in $2n^2+2n$. The vector $\mathbf x$ can be reused.
Since the vector sizes we measure fit into the last level cache, the write backs
are cached. The memory transfer is $2n*8$ bytes to load the vectors and $n^2*8$
bytes to load the matrix. This results in a maximal operational intensity for
large matrices of $\approx 2/8=0.25$ which is confirmed by figure \ref{yonah/dgemv}

For level 3, we chose the 'dgemm' operation,
defined as $C \leftarrow \alpha A B + \beta C $ with double matrices. For an
$n\times n$ matrix, the operation count is $2n^2$ for the two scale operations
and $2n^3$ for the matrix-matrix multiplication and the addition to $C$.

The minimal memory transfer volume is $3n^2*8$ bytes to load the matrices and
$\max(n^2*8-k,0)$ bytes to store the result (write back caching), where $k$ is
the size of the last level cache. In our measurement system $k$ is 2MB.

The maximal operational intensity is $(2n^3+2n^2)/(3n^2*8+\max(n^2*8-k,0))$.
For large $n$ this becomes $(2n^3)/(4n^2*8)\approx n/16$.
This operational intensity cannot be reached since reuse is bound by the cache
size. The intensity achieved in practice depends on the algorithm. In the
following paragraphs we will analyze the triple loop and the blocked version.
To simplify matters, we will ignore the effect of code and small parts of
the working set on the cache.

The triple loop uses the following algorithm:

\begin{lstlisting}[language=C] 
for (long i = 0; i < size; i++) 
for (long j = 0; j < size; j++) 
for (long k = 0; k < size; k++) 
c[i * size + j] += a[i * size + k] * b[k * size + j]
\end{lstlisting}

If $B$ fits into the cache, all matrices have to be loaded exactly once. In this
case the transfer volume is $3n^2*8$ to load the matrices. Since the space
occupied by the reused matrix is not available for writeback caching,
$\max(n^2*8-\max(k-n^2*8,0),0)$ bytes are written. Dropping lower order terms,
the operational intensity is optimal ($n/16$). 

As soon as $B$ does not fit into the cache ($n^2*8>k;n>512$) it is loaded over
and over. In addition, this trashes the the write back cache. The resulting
memory transfer volume is $(n^3+3n^2)*8$ and, for large matrices, the
operational intensity is $1/4$.

For very large matrices, the lines of $A$ do not fit into the cache 
($n*8>k;n>262144$). 
 
The blocked version uses the following algorithm:

\begin{lstlisting}[language=C] 
for (long i = 0; i < size; i += Nb)
for (long j = 0; j < size; j += Nb)
for (long k = 0; k < size; k += Nb)

for (long id = i; id < i + Nb; id += Mu)
for (long jd = j; jd < j + Nb; jd += Nu)
for (long kd = k; kd < k + Nb; kd += Ku)

for (long kdd = kd; kdd < kd + Ku; kdd++)
for (long idd = id; idd < id + Mu; idd++)
for (long jdd = jd; jdd < jd + Nu; jdd++)

c[idd * size + jdd] += 
  a[idd * size + kdd] * b[kdd * size + jdd];
\end{lstlisting}

We chose $N_b=50$ and $N_u=M_u=K_u=2$.
For our discussion, only the blocking with $Nb$ is relevant. It is presumed that
$N_b$ divides $n$. 

If the matrix $B$ fits completely into the cache ($n^2*8<k;n<512$), we again
have a memory transfer volume of $3n^2*8+\max(n^2*8-\max(k-n^2*8,0),0)$. 

The blocking divides the matrices into block lines and block columns, each
containing $\frac{n}{N_b}$ blocks.

If the cache cannot hold the whole matrix but a whole block line fits into the
cache ($\frac{n}{N_b}N_b^2*8=nN_b*8<k;n<5243$), the blocks of $A$ are reused.
Thus $A$ is loaded once while each block of $B$ is loaded $\frac{n}{N_b}$ times,
once per block line of $A$. The memory transfers due to the loads is
$(2n^2+\frac{n}{N_b}n^2)*8=(2n^2+\frac{n^3}{N_b})*8$. The effect of outstanding
writes can be neglected. The writes cause an additional $n^2*8$ bytes to be
transferred. Dropping lower order terms, the operational intensity becomes
$2n^3/(n^3/N_b*8)=N_b/4=12.5$

For large matrices the block lines of $A$ do not fit into the cache ($n>5243$).
Each block of $A$ is loaded once per block column of $B$ and each block of $B$ is loaded
once per block row of $A$. Thus the memory transfer to load $A$ and $B$ is
$2\frac{n}{N_b}n^2*8=2\frac{n^3}{N_b}*8$ and $2*n^2*8$ to load and store $C$.
The resulting operational intensity is $N_B/8=6.25$.

\graphFloat{yonah/mmmOp}{Matrix Matrix Multiplication: Operational Intensity}
When comparing these models for the operational intensity of the triple loop and
the blocked version, we observe a good match for matrix sizes up to $n=200$.
Past this size, the operational intensity of the triple loop rapidly falls to
the value predicted for large matrices, but the drop begins too early. We
predicted the fall when a single matrice occupies more memory than the L2 cache
size, but the measured drop begins at half the cache size. (fig \ref{yonah/mmmOp}).

The model for the blocked version overestimates the measurement results as
well. The initial drop begins too early. The operational intensity for large
matrices is about half the predicted value. 

In attempt to find the reason for the observed behavior we analyzed the TLB
misses. The TLB has 128 entries. For the triple loop we predict
$3*\frac{n^2*8}{4KB}$ TLB misses as long as one matrix fits into 128 pages
($\frac{n^2*8}{4KB}<128;n<\sqrt{128*4KB/8}=256$). If the matrices are larger,
calculating a single element of the result requires reading one element of every
row of $B$. If the rows of $B$ are smaller than one page, multiple accessed
elements will lie in the same page. If the rows are larger, one page is accessed
per row. Thus the number of pages accessed to compute one element of the result
is $\min(n, n^2*8/KB)$. Since the TLB is trashed during accessing a column of
$B$, there are $n^2\min(n, n^2*8/KB)$ TLB misses to compute the $n^2$ result
elements, ignoring the TLB misses due to accessing $A$ and $C$.

For the blocked version, we provide an analysis for a very restricted range
only. If one line of a matrix is larger than one page ($n*8>4KB; n>512$), each
row within a block will lie in it's own page. If each block line fits within one page
($N_b*8<4KB;N_b<512$) and the TLB is trashed during each block multiplication,
there will be $3*N_b$ TLB misses per block multiplication,
$3(\frac{n}{N_b})^3N_b=3\frac{n^3}{N_b^2}$ in total.

\graphFloat{yonah/mmmOpTlb}{Matrix Matrix Multiplication: TLB Misses}
Figure \ref{yonah/mmmOpTlb} shows that the measurement results follow our
predictions.

\graphFloat{yonah/mmm}{Roofline Plot of the Matrix-Matrix Multiplication}
Figure \ref{yonah/mmm} shows the roofline plot of the triple loop, the blocked
version and the two libraries. 

In addition we included the blocked version with
scalar replacement. The scalar replacement has been implemented by specifying
the matrices as 'restrict' pointers to the compiler, hence the name.

\subsection{FFT}
Fast Fourier Transform algorithms 'are of great importance in a variety of
fields, from digital signal processing and solving partial differential
equations to algorithms for quick multiplication of large integers.'
(\cite{FastFourierTransform})

Apart from the simple implementation found in 'Numerical Recipes' (NR)
\cite{Press:2007:NRE:1403886} on page 608, we used the implementations from the MKL \cite{MKL} , FFTW \cite{FFTW}
and Spiral \cite{Spiral}. All four implementations operate on a linear buffer
containing $n$ complex numbers, each represented by two doubles. The
transformation is performed in-place.

\graphFloat{yonah/fft}{Roofline Plot of four FFT implementations}
Figure \ref{yonah/fft} shows the roofline plot of the four implementations. We
see that all except the NR implementation reach a point around
an operational intensity of 0.3 Operations per Byte and a performance  0.4
Operations per Cycle for large input sizes. The NR implementation performs well
initially, but the operational intensity and the performance significantly drop
for large input sizes.

\clearpage
\section{Measurement Tool Architecture}
The tool consists of two main components: The Measuring Core, which performs the
actual measurements, and the Measurement Driver, which controls the core.

High performance code is generally written in C or C++. Therefore the core is
written in that language. To simplify development and maintenance, we tried to
keep the amount of C++ code as small as possible. Therefore the measurement
driver is written in Java.

A measurement is controlled by a measurement specific
routine (one per measurement), which iterates through all parameter points to be examined,
compiles and starts the Measuring Core and processes the results. This should
result in straight forward code for controlling the measurements and, since the
control code is written in Java, a minimal amount of new concepts has to be
learned.

During the development of measurement control routines, it is expected that many
changes do not affect the parameter points. To speed up repeated measurements
after changes to the measurement driver, the measurement results are cached.
Thus, as long as the measurement parameters are not changed, the measurement
does not need to be repeated.

The measurement tool is used to generate and display measurement results. Often,
the measurement results lead to changes to the tool itself. Thus, switches
between using the tool and developing the tool are frequent. To support these
switches, a frontend program is provided. It compiles the measurement driver and
executes it. It can be started using a shell script called "rot". The result
files of a measurement are placed in the current working directory.

\umlDiagram{ToolComponents}

\subsection{Component Collaboration}
Data transfer between the measuring core and the measurement driver is achieved
using serialized objects stored in files. Classes of these objects are
used both from C++ and Java and therefore shared entities. They
are described using XML. Source code for both languages is generated. For
details, see \ref{sec:MultiLanguageInfrastructure} 

The root class for describing a measurement is a MeasurementCommand.
It contains the number of times the measurement should be repeated, as well as
the actual Measurement. The kernels are contained within the workloads, and
rules allow to respond to various events happening during the measurement.

\umlDiagram{MeasurementCommand}

A workload describes what should be run and measured on one core. Each workload
is run within a separate thread, which is optionally pinned to a fixed core. In
this thread, the validation measurers are started, the caches are warmed up, the
additional measurers are started, followed by the main measurer. Then the kernel
is run and the measurers are stopped. 

\umlDiagram{Workload}

During the measurement, events are raised. For example: start of a workload, end
of a workload, start of a thread etc. These events are matched against the event
predicates stored in the rules. If a predicate matches, the action of the rule
is executed.

\umlDiagram{Rule}

The following diagram shows all classes describing a measurement together:

\umlDiagram{MeasurementCommandFull}

A measurement is usually repeated multiple times, to get an idea of the
distribution of the results. Each repetition is called measurement run.
In each run, the outputs of all measurers are collected. At the end of the
measurement, the core serializes the results of all runs into a single file,
which is read by the driver.

\umlDiagram{MeasurementRunOutput}

\subsection{Tour of a Measurement}
In this section, we'll look at the components specific to a measurement. To make
sure you don't get lost, here is the tour map: 
\umlDiagram{MeasurementTourMap}

First, we'll look at the kernel. It is defined in
an XML file:
\begin{lstlisting}[language=XML]
<?xml version="1.0" encoding="UTF-8"?>
<derivedClass
	xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
	xsi:noNamespaceSchemaLocation="../shared.xsd"
	name="TriadKernel" <!-- name of the class -->
	baseType="KernelBase"
	cSuffix="Data"
	comment="Kernel performing a=b+k*d 
		on a memory buffer">
	<field  
		name="bufferSize" 
		type="long" 
		comment="The size of the buffer"/>
</derivedClass>
\end{lstlisting}

Kernels are always derived from KernelBase, hence the derivedClass element on
line 2 and the base type defined on line 6. 

The cSuffix is given as 'Data' on
line 7. This causes the generated class to be named 'TriadKernelData'. The
measuring core implements 'TriadKernel', which derives from TriadKernelData. The
serialization service will instantiate the derived class. This mechanism allows
to use a derived class, optionally with additional code and data, to be used in
the measuring core. This is how the actual algorithm is implemented. We'll look
at this later.

On line 10 starts a field definition. Fields and getters/setters are generated
for the field.

Next comes the class controlling the whole measurement. 
\begin{lstlisting}[language=JAVA]
package ch.ethz.ruediste.roofline.measurementDriver.measurementControllers;

public class TriadMeasurementController implements IMeasurementController {

	public String getName() {
		return "triad";
	}

	public String getDescription() {
		return "runs the triad kernel";
	}

	@Inject
	public QuantityMeasuringService quantityMeasuringService;

	@Inject
	public RooflineController rooflineController;
	
	public void measure(String outputName) throws IOException {
	...
	}
}
\end{lstlisting}
The class implements IMeasurementController and has to be placed in the
measurementControllers package. The measure command will instantiate the class
and call the measure() method. The getName() method returns the name of the
measurement, which is used to identify the measurement.

The two fields with the @Inject attribute are initialzed by the dependency
injection framework when the class is instantiated. The quantity measuring
service allows to measure quantities like operation count, transferred bytes,
performance etc. The roofline controller manages a roofline plot. We will see
how these facilities are used when we look at the body of the measure function:

\begin{lstlisting}[language=JAVA]
public void measure(String outputName){
	rooflineController.setTitle("Triad");
	rooflineController.addDefaultPeaks();

	for (long size = 10000; size < 100000; size += 10000) {
		// initialize kernel
		TriadKernel kernel = new TriadKernel();
		kernel.setBufferSize(size);
		kernel.setOptimization("-O3");

		// add a roofline point
		rooflineController.addRooflinePoint(
			"Triad", Long.toString(size),
			kernel, Operation.CompInstr,
			MemoryTransferBorder.LlcRam);

		// measure the throughput
		Throughput throughput = quantityMeasuringService.measureThroughput(
			kernel, MemoryTransferBorder.LlcRam, ClockType.CoreCycles);

		// measure the operation count
		OperationCount operations = quantityMeasuringService
			.measureOperationCount(kernel, Operation.CompInstr);

		// print throughput and operation count
		System.out.printf("size %d: throughput: %s operations: %s\n", size,
			throughput, operations);
	}

	rooflineController.plot();
}
\end{lstlisting}

First the roofline plot is initialized with the title and the default peaks.
Then, for each buffer size, the kernel is initialized. For each kernel, the
optimization flags used to compile the kernel have to be specified.

Then the roofline controller is instructed to add a roofline point to the plot.
The first argument is the series name, next the label of the data point. Points
with the same series name are connected with a line in the plot. The rest of the
arguments specify the kernel and how the required quantities should be measured.

In the rest of the loop, the throughput and the operation count are measured and
printed to the console. This is an example of how to use the quantity measuring
service.

The last statement of the measure() body causes the plot to be output to a file
in the current directory. This involves the invocation of gnuplot.

During the invocation of addRooflinePoint() and the quantity measuring service a
lot was going on under the hood. First a measurement was created from the
kernel and the measurers required to measure the requested quantities. Then was
checked if there is already a result for the measurement in the cache. If not,
the measurement was serialized, the measuring core was configured, built
and started. Then the result of the core was parsed and stored in the cache.
And finally, the requested quantities were caculated.

The only measurement specific part involved in this process is the
implementation of the kernel. First the header:

\begin{lstlisting}[language=C++]
class TriadKernel : public TriadKernelData{
	double *a,*b,*c;
	
protected:
	std::vector<std::pair<void*,long> > getBuffers();

public:
	void initialize();
	void run();
	void dispose();
};
\end{lstlisting}

The kernel requires three buffers. All declared methods override methods
from the KernelBase. The buffers are allocated and initialized in
initialize() and freed in dispose(). getBuffers() returns the buffers along with
their sizes. This is used to clear the or warm the caches. run() contains the
actual algorithm. 


\begin{lstlisting}[language=C++]
void TriadKernel::initialize() {
	srand48(0);
	size_t size = getBufferSize() * sizeof(double);

	// allocate the buffers
	a = (double*) malloc(size);
	b = (double*) malloc(size);
	c = (double*) malloc(size);

	// initialize the buffers
	for (long i=0; i<getBufferSize(); i++){
		a[i]=drand48();
		b[i]=drand48();
		c[i]=drand48();
	}
}

std::vector<std::pair<void*, long> > TriadKernel::getBuffers() {
	size_t size = getBufferSize() * sizeof(double);

	std::vector<std::pair<void*, long> > result;
	result.push_back(std::make_pair((void*) a, size));
	result.push_back(std::make_pair((void*) b, size));
	result.push_back(std::make_pair((void*) c, size));
	return result;
}

void TriadKernel::run() {
	for (long p = 0; p < 1; p++) {
		for (long i = 0; i < getBufferSize(); i++) {
			a[i] = b[i] + 2.34 * c[i];
		}
	}
}

void TriadKernel::dispose() {
	free(a);
	free(b);
	free(c);
}

\end{lstlisting}

\subsection{Multi Language Infrastructure}
\label{sec:MultiLanguageInfrastructure}
The shared entities are used from both C++ and Java. To avoid having to
manually synchronize two versions of the same class, the source code for the C++
and the Java implementation is generated from an XML definition by the Shared
Entity Generator. The XML definition contains class and field definitions only,
no code. If class specific code is needed, it has to be implemented separately for each language and merged with the field definitions using
inheritance.

The shared entity definitions, written in XML, are parsed using a
serialization library called XStream. XStream maps classes to an XML
representation. The classes used to define the shared entities are shown
in \umlRef{MultiLanguageClassDefinition}.

\umlFloat{MultiLanguageClassDefinition}{Classes representing a multi language
class definition}

The following is an example of a class definition:
\begin{verbatim}
<?xml version="1.0" encoding="UTF-8"?>
<class name="MultiLanguageTestClass" 
  cBaseType="MultiLanguageObjectBase"
  javaBaseType=""
  comment="Multi Language Class used for unit tests">

  <field 
    name="longField" 
    type="long" 
    comment="test field with type 'long'"/>
  <list  
    name="referenceList" 
    type="MultiLanguageTestClass" 
    comment="list referencing full classes"/>
  <field 
    name="referenceField" 
    type="MultiLanguageTestClass" 
    comment="field referencing another class"/>
</class>
\end{verbatim}

After the definitions are loaded, Velocity templates are used to generate all
source code.

A normal entity has a C and a Java base type. The C base type has to directly or
indirectly inherit from SharedEntityBase, which is a polymorphic class.
This allows to use the RTTI (RunTime Type Information). Java base types have no
such constraint (due to the implicit common base class Object). The base types
are just included in the generated source code, but have no other effect on the
code generation.

A derived entity names another entity as base type. The C and
Java base types are set to that class. The fields of the base class are included in
the serialization process. If just the C and Java base types would be set to the
shared entity used as base class, the generated class would still derive
from the base class, but the fields of the base class would not be included in
the serialization process.

Often it is necessary to mix hand written code with the generated code. To
support this, a suffix can be specified, which is added to the name of the
generated class. Only the name of the generated class is affected, not the type
name used for references to the class. A class named without the specified
suffix has to be provided manually, and should derive from the generated class.
Any additional code as well as additional fields can be included in the hand
written class.

The class definitions and the generated code is located in the Multi Language
Classes project. The generated Java code is linked by the Measurement Driver
project. The generated C code is linked by the Measuring Core. The following
Diagram shows these dependencies:

\umlDiagram{MultiLanguageCodeGeneration}

\subsubsection{Serialization and Deserialization}
Along with the source code for each class, a service serializing and
deserializing multi language objects to/from a simple text based format is
generated for both languages. It supports the following primitive types:
\begin{itemize}
\item double
\item integer
\item long integer
\item boolean
\item string
\end{itemize}

References to other shared entities are supported. The serializer can handle
general object graphs.

Lists containing one of the supported primitive types as well as containing
references to other shared entities are supported.

The service implementations for both languages follow the same structure. Each
has two methods, one for serialization and one for deserialization. 

The serialization method receives an object and an output stream. The  method
body contains an if for each known serializable class, which checks if the class
of the object received is equal to the serializable class. If true, the value of
all fields of the class and it's base classes get serialized. For references,
the  serialization method is called recursively with the same output stream and
the  referenced object as parameters.

The deserialization method works analogous to the serialization method. It
receives an input stream. The method body contains an if for each known
serializable class, which checks if the next line of the input names  the
serializable class. If true, a new instance of the class is created and the
value of all fields are read from the input and set on the created instance,
including all fields declared in a base class. If a reference is encountered,
the deserialization method is called recursively with the same input, and the
returned instance is used as field value.

\subsection{Frontend}
The measurement tool is a console tool controlled using command line options.
Measurement results are either directly dispalyed on the console, dumped to a
data file or processed, usually for generating a graph. The graph is typically
stored as a file. But unlike normal tools, the source code is expected to change
frequently, and the user likely switches often between coding and using the
tool.

To support this usage pattern, the build process has been integrated into the
normal tool operation. The frontend is used to first trigger the build process
and then invoke the measurement driver. Otherwise, the user would have to keep
to console windowses open, one for building and one for measuring, and not to
forget building to see the changes made to the source code.

After building, the frontend starts the the measurement driver, forwarding it's
own command line. Certain flags are used to control the operation of the
fronted. These are not forwarded.

The frontend has a configuration system. The known configuration keys are
defined at the top of the Main class. There are three configuration sources. The
default configuration stored in a configuration file. It contains templates
which are expanded during the build process. The result is included as resource
in the generated JAR file. Flags of the default configuration can be overwritten
using a user configuration file, which is located by default under
"~/.roofline/frontendconfig". This location can be changed using a command line
argument. Finally, the command line options known by the frontend are used to
modify the configuration flags after they have been loaded. 

\subsection{Measurement Driver}
For the design of the measurement driver, we used software engineering best
practice, namely unit testing (junit), mocks (jmock), and dependency
injection (guice). Further a domain model (DOM), controllers, repositories and
stateless services as described in \cite{evans2004domain}.
Describing all these concepts lies beyond the scope of this report. It is
assumed that the reader has a basic understanding of the mentioned concepts.

The following diagram shows an overview of the driver:

\umlDiagram{measurementDriver/MeasurementDriverOverview}

In the following paragraphs, we will have a quick look at the look at the
different parts. 

The entry point of the driver is the \class{Main} class. First, the dependency
injection framework is initialized. This is accomplished using the
\class{MainModule}. It's \method{configure()} method uses the
\class{ClassFinder} to find all compiled classes and configures how they are
instantiated, mainly based on naming conventions.

Then the command line arguments are parsed. If command line auto completion is
desired (indicated by the '-autocomplete' flag), the auto completion process
starts. 

Otherwise the configuration is initialized, using the flags specified at the
command line and the configuration files (default configuration stored in the
jar and user configuration from the home directory of the user).

The last step in the initialization sequence is to set up log4j, the logging
framework. 

Then the class for the command given on the command line is instantiated and
the \method{execute()} method on the resulting \class{ICommand} is called.

When a 'measure' command is given, a \class{MeasurementCommandController} is
instantiated. The command controller looks a the next command line argument,
instantiates the corresponding measurement controller and calls the
\method{measure()} method.

The measurement controller will typically create multiple \class{Measurement}s
with different parameters. The \class{ParameterSpace} facilitates iterating
over all possible parameter combinations. Each parameter is associated to an
\class{Axis}. For each axis, one or multiple values can be given. The
\method{getAllPoints()} returns a \class{Coordinate} for each possible value
combination. Itearating over the points in the space, the measurement controller
can construct a measurement for each point. 

The results of the constructed measurements can be either printed to the
console, or stored in one of various \class{Plot}s. When all data is gathered,
the plot can be rendered and written to an output file using the
\class{PlotService}.

Although it is possible to directly create the \class{Measurer}s required to
measure something, most of the time the intent is to measure a certain
\class{Quantity} (\class{OperationCount}, \class{TransferredBytes},
\class{Performance} etc). The \class{QuantityMeasuringService} can be used to
obtain a \class{QuantityCalculator} for a quantity. The calculator can be
queried for the list of measurers which are required to calculate the quantity.
When the results of all required measurers are known, they can be passed to the
calculator, which will return the desired quantity. In addition, the quantity
measuring service provides convenience methods for working with the quantity
calculators.

Once the \class{Measurement} is constructed, the \method{measure()} method of
the  \class{MeasurementAppController} is used to perform the measurement. First
it is checked if a cached result is available for the measurement (using the
\class{CacheService}). If not, the \class{MeasuringCoreService} is used to build
the core for the measurement and to start the core.

The \class{Kernel}s can define macros. During build preparation, all macro
definitions present in the measurement are collected and written to generated
header files within the core.

During the execution of the measurement driver, the run time used for the
various tasks is collected using the \class{RuntimeMonitor}. At the end of the
execution, the times spent for the tasks is printed to the console.


\subsubsection{Dependency Injection Configuration}
Generally, a convention over configuration approach was chosen for the
configuration of the dependency injection. The conventions as well as optional
exceptions are defined in the MainModule. 
The conventions are:
\begin{description}
\item[Services] all classes in the services packages are bound to themselves as
singletons
\item[Repositories] all classes in the repositories packages are bound to
themselves as singletons
\item[Application Controllers] all classes in the 'appControllers' package are
bound to themselves as singletons
\item[Measurement Series] all classes deriving form IMeasurementSeries in the
measurement series package are bound to the IMeasurementSeries interface
annotated with their name
\item[Commands] all classes deriving from ICommand in the commands package are
bound to the ICommand interface annotated with their name
\item[Measurement Controllers] all classes deriving from IMeasurementController
in the measurement controller package are bound to the IMeasurementController
interface annotated with their name
\end{description}

\subsubsection{Configuration}
The design goal was to create a configuration system which
\begin{itemize}
\item allows to set configuration flags from the command line and from
configuration files
\item can manage some form of comment for the flags
\item makes the available flags transparent
\item supports user specific configuration
\end{itemize}

\umlDiagram{measurementDriver/Configuration}
The central class of our solution is the \class{ConfigurationKey}. A
configuration key contains a string key which identifies the configuration flag it represents. In
addition, it contains a description and the default value of the flag. It has a
template parameter which defines the data type of the flag. This removes the
necessity to use type casts when reading configuration flags. Configuration keys
should be stored in public static variables. The help command scans all classes
of the measurement driver for such configuration keys and prints the key string
and the description. After the configuration is loaded, it is checked if a
configuration key is defined for each configuration flag specified. If a
configuration key for a flag is missing (or more likely, a configuration flag
has been misspelled in the configuration) an error is generated.

The values associated with configuration keys are stored in
\class{Configuration}s. Configurations can be chained together using the parent
links. If no value is found in a configuration or all of its ancestors, the
default value stored in the configuration key is used. 

The state of a configuration kan be saved on a stack using \method{push()} and
restored using \method{pop()}. All modifications to a configuration after a push
are undone by the pop. This can be used to temporarily change the
configuration.

The following paragraphs describe the sources of configuration flag definitions
in order of decreasing precedence. 

The command line is scanned for arguments starting with a dash. Such arguments
are expected to be in the form of "-$<$flag key$>$=$<$value$>$" and specifies
that the configuration with the specified flag key should have the specified
value. Configuration flag definitions on the command line have highest
precedence.

Next come two configuration files. They both have the same format: each line
consists of the flag key, followed by an equal sign and the flag value. 

The first file is the user configuration file. By default it is located under
\textasciitilde/.roofline/config, but this can be changed using the
"userConfigFile" configuration flag, in particular by overwriting the flag on
the command line.

The second file is the default configuration. It is located in the source code
of the measurement driver, and can be loaded from the classpath. It contains
some placeholders, which are expanded during the build process.

Finally, the flag definitions with lowest precedence are the default values
given in the configuration keys.

\subsubsection{Auto Completion}

\subsubsection{Commands}
A command is represented by a class deriving from ICommandController and should
be placed in the commands package. A command has a name and a description, which
should be the return value of the getName() respectively getDescription()
methods of the command. The measurement driver expects a command name as first
argument. The name is matched against the names of all available commands. If a
command matches, the execute() method of a new instance of the corresponding
class is called with the remaining command line arguments as parameter.

\subsubsection{Measurement Controllers}
The operation of the measurement driver is controlled by the measurement
controllers. They define which measurements to perform and how to process the
output. The measure command instantiates a measurement controller and calls the
measure() method.

\subsubsection{Parameter Space}
When implementing measurement controllers, one often has to iterate over all
possible combinations of some parameters. The \class{ParameterSpace} was
designed to support this. 

Every parameter is identified by an \class{Axis}. For
each axis, one or multiple values are specified. After the desired values are
specified, all possible parameter combinations can be generated, represented by
\class{Coordinate} objects. The points are generated implicitely when iterating
over the space.

Example:
\begin{lstlisting}[language=Java]
space.add(systemLoadAxis, SystemLoad.Idle);
space.add(systemLoadAxis, SystemLoad.DiskOther);
space.add(systemLoadAxis, SystemLoad.DiskAll);
space.add(systemLoadAxis, SystemLoad.AddOther);
space.add(systemLoadAxis, SystemLoad.AddAll);

space.add(clockTypeAxis, ClockType.CoreCycles);
space.add(clockTypeAxis, ClockType.ReferenceCycles);
space.add(clockTypeAxis, ClockType.uSecs);

for (Coordinate coordinate : space) {

	ClockType clockType 
		= coordinate.get(clockTypeAxis);
	SystemLoad systemLoad 
		= coordinate.get(systemLoadAxis);
	...
}			
\end{lstlisting}

To faciliate the initialization of measurements, the classes of the measurement
description have an \method{intialize()} method which takes a coordinate as
parameter. Depending on the kernel or measurer at hand, some fields are
set to the value of an axis given by the coordinate.

The most common axes are defined in the \class{Axes} class.

\subsubsection{Retrieving Outputs}
To process the results of a measurement, it is frequently necessary to
retrieve the output of a specific measurer. 

The straight forward approach would be to give each measurer an unique id, and
to store the id of the measurer with the measurer output. Measurers are newly
created with each invocation of the measurement driver, possibly leading to new
ids. But for caching, the ids of the measurers do not matter.

To overcome these problems, two ids are generated for each measurer. One
identifier uniquely identifying each instantiated measurer. And an id which is
unique within one measurement. When loading a result from cache, the unique
identifiers of the loaded result are set to the identifiers of the measurement
at hand.

To retrieve the output of a measurer, the \class{MeasurementResult} and
the \class{MeasurementRunOutput} provide several methods which take a measurer
as argument and return it's output.

\umlDiagram{measurementDriver/RetrievingOutputs}

\subsubsection{Plotting}

\subsubsection{The MeasurementAppController}
The measurement application controller is the entry point for performing
measurements. It is the sole client to the \class{MeasuringCoreService}, which
provides the low level control over the measuring core, and keeps track of the
measurement the core is compiled for. 

In addition, it uses the \class{MeasurementHashRepository} to keep track of
measurements which have equal measuring cores.

The main method of the controller is measure(). Functionality in pseudo Code:

\begin{minipage}{\textwidth-18pt}
\begin{lstlisting}[language=Java,style=pseudoCode]
MeasurementResult measure(measurement, numberOfRuns)
"prepare measurement"
runOutputs=[]
if (useCachedResult || "measurement has been seen")
	loaded="load stored results"
	if (loaded!=null)
		if (!shouldCheckCoreHash
			|| currentCoreHash==loaded.coreHash)
			runOutputs=loaded
			
if ("more results needed")
	newResult=performMeasurement()
	"merge and store loaded and new run outputs"
	
"build and return MeasurementResult with the desired number of run outputs"
\end{lstlisting}
\end{minipage}

The method first tries to load a measurement result from the result cache. If
not enough run outputs are loaded (or none at all), the measurement is performed
to get the remaining measurement run outputs.

Finally, a measurement result with exactly the requested number of runs is
constructed and returned.

It is possible to disable loading stored results by setting the useCachedResults
configuration key to false. The measurement is performed and existing results
are overwritten.

The hash code of the core is stored along with the measurement result. This
allows to check if the currently compiled core is equal to the core which was
used during the measurement. By default, if the core changed since the results
were generated, the results are not used and new results are generated using the
current measuring core. By setting shouldCheckCoreHash to false, this check can
be skipped.

Preparing and building the measuring core are expensive operations in term of
runtime. Therefore, the measurement application controller keeps track of as
much information about the measurements and the cores needed to perform them as
possible. The \class{MeasurementHashRepository} is used for this purpose. It
has the following internal Model: 
\umlDiagram{measurementDriver/MeasurementHashRepositoryModel}

The \class{Core} class is private and does not leave the repository.

The model is exposed through the following methods
\begin{itemize}
  \item areCoresEqual(measurementA, measurementB): bool
  \item setHaveEqualCores(measurementA, measurementB)
  \item setCoreHash(measurement,coreHash)
  \item getCoreHash(measurement): CoreHash
\end{itemize}

Before building, the controller asks the repository if the core for the new
measurement is the same as the currently built one. (using
\method{areCoresEqual()}) If the cores are the same, no building is required.

During the build preparation the controller and the \class{MeasuringCoreService}
monitor changes to the core. If no changes were necessary, the repository is
notified using \method{setHaveEqualCores()}. The cores of the two measurements
are merged.

When a core hash is required (for example to check if the current core is the
same as the one used to generate a stored result), the controller first asks the
repository for it using \method{getCoreHash()}. If the hash is not known, the
core is built for the measurement, the hash is calculated from the core and
stored in the repository using \method{setCoreHash()}. This could again lead
to a merging of two cores, if a core with the same hash is present already.

Building the measuring core can become necessary when the core hash of a
measurement has to be known, or when a measurement is to be performed. This is
reflected in the call graph of the methods within the application controller:

\umlDiagram{measurementDriver/MACCallGraph}

Since it makes sense that services can start measurements, the
\class{MeasurementService} provides a \method{measure()} method. The service knows an
instance of \class{IMeasurementFacility} which provides \method{measure()}, and
forwards all calls to its own \method{measure()} method to the measurement
facility. The facility is is implemented by the \class{MeasurementAppController}
controller. Therefore the service ultimately forwards all calls to
\method{measure()} to the application controller.

\umlDiagram{measurementDriver/measureCallLift}


\subsubsection{Architecture Specific Behavior}
The measurement driver supports multiple system architectures. Currently, the
system architecture is identified by the available PMUs. When a performance
event is to be read, a list with the event for each architecture is passed to
the \method{getAvailableEvent()} method of the \class{SystemInfoService}. The
method returns the available event. In other cases, the presence of a PMU is
checked directly.

For further development, it might become beneficial to use the specification
pattern. 

\subsubsection{Preprocessor Macros}
Preprocessor macros are used to allow flexible compile time parameterization of
the measuring core. The macros are defined by the measurement driver. Before the
compilation of the measuring core, the measurement driver writes the definition
of each macro to a separate include file. This allows the build system to track
macro definition changes for and to recompile only the required parts.

In the measurement driver, each macro is identified by a macro key, which
contains the macro name, a description and the default value. The macro
definitions are stored in classes deriving from MacroDefinitionContainer. The
classes should define macro keys by placing them in private static variables. To
access the macro definition, getters and setters have to be provided. 

When the measuring core is configured to perform a measurement, the macro keys
are collected from the classes of the measurement driver using reflection. Then
the macro definitions are extracted from the measurement definition and
referenced objects. If no definition is given for a macro, the default
definition found in the macro key is used. If contradicting definitions are
found, an error is raised.

\umlDiagram{measurementDriver/MacroDefinitions}

\subsubsection{Measurement Result Caching}
The measurement controllers mix the definition of the measurement parameters and
the processing of the output. Thus, if the output processing logic needs to be
modified, the measurements have to be performed again. This causes a delay,
which is avoided by caching the measurement results. 

All parameters of a measurement are contained within the measurement description
and the referenced objects. Therefore, if the measurement description is
identical to a measurement description of a previous measurement, the result of
the previous measurement can be reused. 

The cache mechanism works using a hash function on the XML representation of the
measurement description. After a measurement has been performed, a file named
after the hash value of the measurement description of the measurement is created
and the measurement results are stored therein. Before a measurement is
performed, the hash value is computed. If a corresponding file is found, the
previous measurement results are reused.

\subsection{Measuring Core}
The measuring core is based on the object graph constructed by the driver. The
classes are extended with code and data.

\subsubsection{Core Architecture}
To fully utilize multi core systems, applications have to be implemented with
multiple threads or processes. Measuring such applications is considerably more
difficult than measuring single threaded applications. If the thread management
is implemented specifically for the measurement at hand, the measuring code can
be weaved into it by hand. But if the threads or processes are created within
legacy or closed source code, the measuring tool has to take care of detecting
the creation of threads and install the necessary measurers. For the roofline
measuring tool we will only consider multi threaded applications.

To gain full control over the kernel code, the measurement tool starts the
kernel within a child process. The parent process attaches to the child using
ptrace. This causes the child to be stopped when certain events occur and the
parent is notified. The events include thread creation and breakpoints.

Every kernel thread can raise events at any point. The events include thread
creation, breakpoints, starting and stopping of workloads etc. Whenever an event
occurs, the rule list has to be searched and the matching actions have to be
executed. The rule list is always searched in the thread which raised the event.
If the event caused the thread to be stopped and the parent thread to be
notified, the parent restarts the thread with a notification of the event which
occurred. The thread will search the rule list and continue execution.

It is important to distinguish the events from notifications in this context. An
event is handled by the rule list of the child and typically generated by the
child process as well. A notification is used to communicate between the child
process and the parent process. However, the parent can notify (using a
notification) the child of a certain observation, which causes the child to
generate an event.

The rule list is always searched in the thread generating the event. If an
action has to be executed in another thread, it has to be queued using
\method{ChildThread::queueAction()}

The technically most challenging problem is to interrupt another thread and make
it execute some action. There are two approaches to this problem, either using a
signal handler of the thread or using ptrace to call code within the thread.
Since installing a signal handler in a thread could cause unwanted side effects,
we use the ptrace approach. SIGTRAP is sent to the target thread, which will
cause it to be stopped. The parent modifies the thread state of the stopped
thread. When the thread resumes execution, it executes some event handling code
and then return to the location the thread was interrupted.

\subsubsection{Child Thread States}
\umlDiagram{measuringCore/ChildState}
The parent process manages a state for each child thread. When a child thread
starts, ptrace will immediately stop it with SIGSTOP. If the notification system
is ready, a ThreadStarted notification will be sent to the child.

While a notification is beeing processed by the child, it's state is set to
processing. When no more notifications are pending, the state is set to running.
If a notification is queued from one thread to another and the state of the
receiving thread is running, a SIGTRAP is sent to the receiving thread and the
state of the receiving thread is set to stopping. The stopping state indicates
that the thread will stop eventually.

A thread can exit at any time. When this is detected by the parent process, the
state and the notification queue are erased.

\subsection{Thread Representation in the Child Process}
In the child process, threads are represented as \class{ChildThread} objects.
Since every \class{Workload} runs in it's own thread, a child thread is
associated with every workload. 

When a new thread is spawned, it will be stopped by ptrace. The parent sends the
ThreadStarted notification to the child. In the handler, the child will
instantiate the \class{ChildThread}. If the started thread is a workload thread,
the startup routine of the workload will associate the instantiated child thread
with the workload.

Actions can be sent from one thread to another using
\method{ChildThread::queueAction()}. 

\subsubsection{Building}
Each measurement can be performed with different compiler optimization flags and
macro definitions. Therefore, the measuring core has to be rebuilt for each
measurement, which makes rebuilding the measuring core a frequent operation. It
should therefore be as fast as possible. This is achieved by carefully tracking
all build dependencies and by using ccache. 

CCache is a compiler cache. Whenever the compiler is run, ccache hashes all
input files, together with the compiler flags. It then checks if it's cache
already contains an entry for the hash value. If this is not the case, ccache
runs the compiler and stores the output together with the hash value of the
input in it's cache. If the hash value is present already, it does not run the
compiler but uses the compiler output stored in it's cache. This considerably
speeds up recompilations.

But ccache still has to build the hash values and copy the compiler output,
which takes some time. This is where tracking the build dependencies comes in.
The following parameters can change between measurements:
\begin{description}
\item[Macro definitions]
Each macro definition is stored in a separate file, which is only updated by the
measurement driver if the macro definition changes. Every source file which
needs a macro definition includes the corresponding file. These inclusions are
tracked and allow to only recompile the affected source files.
\item[Compiler flags] 
Each kernel specifies it's own optimization flags. The compiler flags used for
the rest of the measuring core do not affect the measurement results. The compiler
flags are stored in a separate file. Whenever it is changed, the parts affected
are recompiled.
\item[Compiled Kernels]
For each measurement, only the kernels actually used are compiled. The
measurement driver writes the kernel names into a separate file. The child
binary is recompiled when it changes.
\end{description}

The build process is controlled using the gnu make utility \cite{make}. Make
automatically determines which parts of a program have to be recompiled, based
on rules stored in a makefile. Each rule consists of target files, prerequisite
files and a recipe. Make checks the modification times of the target and the
prerequisite files. If any prerequisite is newer than any target, the recipe is
executed in order to update the target files. The recipe is a sequence of shell
commands.

The following diagram shows how the source files are categorized and compiled:

\umlDiagram{MeasuringCoreBuild}

The makefile used for the measuring core first instructs make to use the find
utility to get a list of all source files (with .cpp extension) in the
measuringCore/src and measuringCore/generated directories (ALL\_SOURCES).

Using the filter functions of make, the kernel sources are set to the subset of
ALL\_SOURCES which is located in src/kernels or generated/kernels. These are all sources related to kernels.
(ALL\_KERNEL\_SOURCES).

The sources of the parent process are the subset of ALL\_SOURCES which is
located in src/parent(PARENT\_SOURCES).

The source files of ALL\_SOURCES not contained in ALL\_KERNEL\_SOURCES or
PARENT\_SOURCES are stored in CHILD\_SOURCES.

The names of all present kernels are stored by the measurement driver in
generated/kernelNames.mk. For each of the kernels named there, the source file
named after the kernel and all source files in the subdirectory named after the
kernel collected in a variable (KERNEL\_SOURCES\_\$(kernel)). They are compiled
with the optimization flags of the compiler, wich are stored under
generated/kernelOptimization.

The sources of all current sources are collected and form, together with the
CHILD\_SOURCES, the sources of the child process.

The parent process is compiled from the PARENT\_SOURCES.

There is a rule without recipe with the kernel objects as target and the file
containing the measurement specific optimization flags as prerequisite. This
causes the file containing the optimization flags to be added as prerequisite
for each kernel object file.

In the C programming language, it is possible to include other files in a source
file. Of course, the compiled code depends on the contents of the included
files, too. To track these build dependencies, the compiler is instructed to
generate rules without recipes with the object file as target and the source
file together with the included files as prerequisites. The generated rules are
stored in .d files in the build directory and are included in the makefile.

If special compilation flags are required for a source file, a rule should be
added near the end of the makefile.

\subsubsection{System Initialization}
We chose a modular approach to initialize the measuring core. Whenever a system
part needs to run code when the program starts or shuts down, it can instantiate
a class derived from SystemInitializer.

This is preferably achieved by declaring a global static variable named dummy
in a .cpp file. Example:
\begin{verbatim}
// define and register a system initializer.
static class FooInitializer: public SystemInitializer{
  void start(){
    // code to be executed on startup
  }

  void stop(){
    // code to be executed on shutdown
  }
} dummy;
\end{verbatim}

It is important to give every initializer subclass an individual name. Use the
name of the file the initializer is declared in as prefix. If two initializer
classes have the same name, they don't work correctly (instances of the wrong
classes are created)

Whenever a SystemInitializer is instantiated, the instance is registered in a
static global list. On system startup and shutdown, the start() respective
stop() method of all registered SystemInitializers is called.

\section{How To}
\subsection{Installation}
see INSTALL file in tool directory

\subsection{Create a New Kernel}
First, you have to create an XML description of the new kernel in
sharedEntities/definitions/kernels. Look at some of the other files in that
directory and choose one as starting point for your own. Copy the chosen file
and give it the name of your kernel. It must end with 'Kernel'. The file name is
taken as class name for your kernel description.

Next, run 'rot help' to generate the source code from your XML description. In
case your class defines a 'javaSuffix', the compilation following the source
generation will fail. Create the Java code of your kernel class in
measurementDriver/src/ch/ethz/ruediste/roofline/sharedEntities/Kernel and make
it inherit from the generated class with the suffix. (see other kernels for
examples)

The Java part of your kernel is now ready. You can use it in a measurement. But
we are still missing the implementation in the measuring core. To implement the
kernel, it is indispensable to specify a 'cSuffix' in your kernel description.

Create a class in measuringCore/src/sharedEntities/kernels, include the
generated header file (filename contains the suffix) and derive from the
generated class. Add fields for all the data buffers you plan to use
(if any). Override and implement the following methods:
\begin{description}
\item[\method{initialize()}] Allocate and initialize the required buffers.
\item[\method{getBuffers()}] Return the list of all buffers along with their
size. This is used to automatically clear or warm the caches.
\item[\method{run()}] Run the kernel.
\item[\method{dispose()}] Free all buffers
\end{description}

In case you have additional code, you can place it in a subdirectory with the
same name as your kernel, without the 'Kernel' suffix. All '.cpp' files will be
compiled and linked with the measuring core.

\subsection{Use the Driver as a Library}
You can use the measurement driver as a library. First, run './gradlew
--daemon measurementDriver:runnableJar' to create a jar containing the driver
together with all dependencies. It will be located in
'measurementDriver/build/distributions/measurementDriver.jar'. Include the jar
in your project.

During the startup of your application, call LibraryMain.initialize(). Now you
can use the driver. You can directly instantiate entities and use
\method{Instantiator.instance.getInstance()} to retrieve service instances.

\subsection{Create New Measurement}
\begin{itemize}
\item create new class in measurementDriver/measurements
\item implement IMeasurement
\end{itemize}

\subsection{Add Configuration Key}
The configuration is used to set various flags in the measurement driver.
\begin{itemize}
\item add public static field of type ConfigurationKey to any class within the
measurement driver.
\end{itemize}

\subsection{Generate Annotated Assembly}
\begin{itemize}
\item in Eclipse, hit build (Ctrl+B)
\item change the kernel header file (make it recompile)
\item build again
\item from the console window, copy the compiler invocation for
MeasurementSchemeRegistration.cpp
\item open a terminal and go to tool/measuringCore/Debug.
\item paste the compiler invocation
\item insert "-Wa,-ahl=ass.s", check optimization flags
\item issue command
\item the annotated assembly code can be found in tool/measuringCore/Debug/ass.s
\item open the annotated assembly code in Eclipse
\end{itemize}

\section{===WORK IN PROGRESS===}
\section{Measuring Execution Time}
\begin{itemize}
\item 0x40000000   UNHALTED\_CORE\_CYCLES 30Ah CPU\_CLK\_UNHALTED.CORE
\item 0x40000025   UNHALTED\_REFERENCE\_CYCLES 30Bh CPU\_CLK\_UNHALTED.REF
\item 0x400000ad   THERMAL\_TRIP 3Bh C0h 
\item 0x400000ae   CPU\_CLK\_UNHALTED
\item 0x400001a9   ix86arch::UNHALTED\_CORE\_CYCLES
\item 0x400001aa   ix86arch::INSTRUCTION\_RETIRED
\item 0x400001ab   ix86arch::UNHALTED\_REFERENCE\_CYCLES 
\end{itemize}

\subsection{Experiments}
To gain insight into the accuracy and precision of the timers available, we run
the addition kernel with an exponentially increasing iteration count. The
largest iteration count should result in a runtime of about 0.5s. We expect the
precision to be high for small iteration counts, a peak variance at around 5 ms
due to the task switches, and an increasing precision for higher iteration
counts. The accuracy of the minimum should be high for small iteration counts,
and include a small overhead if multiple task switches occur during the
measurement.

\subsection{Experiment 1}
The goal of this experiment is to show the effects of task switches and disk IO
on measuring execution time. As far as possible, we'd like to exclude the
effects of shared memory bandwidth. 

We measure the arithmetic add kernel. Iteration count is increased until total
execution time is about half a second. We measure at both the minimal and
maximal CPU frequency. We measure on an idle system, when another arithmetic
kernel runs or when heavy disk IO is performed. We let the system load threads
run on all of the system's CPU. Separate measurements for core cycles, bus
cycles, gettimeofday(), times (user time)

We estimate the execution time for one iteration. We take the time for the
highest iteration count where we can still get measurement runs without any
interrupts. This time is divided by the respective iteration count.

Graphs (idle, disk, arithmetic): 

\begin{itemize}
\item X: exp exec time [ms, cycles] Y: rel error V: box plots
\item X: exp exec time [ms, cycles] Y: ints V: box plots
\item X: exp exec time [ms, cycles] Y: task sw V: box plots
\end{itemize}

\subsection{Experiment 2}
Investigate the effects of task switches on a memory intensive kernel. Repeat
experiment 1 with both a memory load and write kernel

\subsection{Experiment 3}
Measure the influences of shared memory bandwidth on the runtime. Depending on
the system architecture, we expect to measure shared bandwidth effects. We
measure a memory load and a memory write kernel. We measure on idle system and
while running memory load and write kernel on the other CPUs. Buffer sizes are
larger than cache size. Measure core cycles. Measure at min and max frequency.
Buffer size chosen such that execution time is around half a scheduling time
segment.

One plot per kernel combination (load/load, load/write, write/load, write/write)
\begin{itemize}
\item X: cpu map Y: time[cycles]
\end{itemize}


\subsection{Experiment 4}
Unfortunately, even on a very lightly loaded system, any task can be executed on
any other core. Caused by the shared memory bandwidth, this could affect short
running kernels, and result in wrong measurement results of a whole measurement
series. We show this by starting two memory intensive, short running kernels
simultaneously. Solution: introduce sleeps between measurement runs, to make
sure they are independently measured.

with and without sleep: X: measurement number Y: time[cycles]

\subsection{Experiment 5}
For long running measurements, it is sufficient to make sure that the average
system load is low. The side effects of code running on other kernels will be
smoothed out. This is shown on various average loads (none, 1 2 3 5 10 20 40 50
75 100)

X: system load Y: rel error
X: system load Y: variance

We measure using the following counters:
- core cycles
- bus cycles
- gettimeofday()
- times(): user time

We measure the following kernels:
+- arithmetic:add
*- mem: load
*- mem: write

We measure at 
*+- max frequency
+- min frequency

We measure use the following kernels to generate system load
+- arithmetic: add
*- mem: load
*- mem: write
+- io: file 
- io: network
- io: graphics

We measure at different load levels (*+none, 25, 50, 75, *+100)

Measure at different load switch durations (0.1,1,5,10,15,50,100 ms)

We measure with any core combination.
same core / +core2
core *+0 *+1 *+2 *+3



\section{Measuring Memory Traffic}
\begin{itemize}
\item 0x40000027   LLC\_MISSES
\item 0x40000040   SSE\_PRE\_EXEC 07h03h SSE\_PRE\_EXEC.L2
\item 0x40000059   L2\_DBUS\_BUSY\_RD
\item 0x4000005c   L2\_LINES\_IN  24h
\item 0x4000006a   L2\_M\_LINES\_OUT  27h
\item 0x400000cb   SSE\_PRE\_MISS 4Bh 00h/01h/02h
\item 62h BUS\_DRDY\_CLOCKS
\end{itemize}

\subsection{Experiments}
To gain insight into the accuracy and precision of the memory transfer methods,
we run the addition kernel with an exponentially increasing iteration count. The
largest iteration count should result in a runtime of about 0.5s. We expect the
precision to be high for small iteration counts, a peak variance at around 5 ms
due to the task switches, and an increasing precision for higher iteration
counts. The accuracy of the minimum should be high for small iteration counts,
and include a small overhead if multiple task switches occur during the
measurement.

\subsection{Experiment 1}
The first series of measurements is about the overhead of task switches on the
measured memory transfer. We measure an arithmetic kernel and mem load and
write, with buffer sizes which fit into the cache and which do not fit. We
measure on an idle system, and with an arithmetic kernel on the other CPUs.

X: iteration count Y: rel error 
X: iteration count Y: ints
X: iteration count Y: task switches

\subsection{Experiment 2}
The second measurement series is about the separation of measuring transfer
volume of different threads. We measure mem load and write with large buffers.
We measure on an idle system and with mem kernels on the other CPUs. Execution
time: about .5 s

X: cpu map Y: rel error

We measure using the following counters:
- bus transactions
- cache misses / writebacks / non-prefetching load/store instructions

We measure the following kernels:
- arithmetic:add
- mem: load: small 
- mem: write: small
- mem: load: large
- mem: write: large

We measure at 
- max frequency

We measure use the following kernels to generate system load
- arithmetic: add
- mem: load
- mem: write
- io: file 
- io: network
- io: graphics

We measure at different load levels (none, 25, 50, 75, 100)

Measure at different load switch durations (0.1,1,5,10,15,50,100 ms)

We measure with any core combination.
same core / +core2
core 0 1 2 3



\section{Effects Affecting the Measurement of Multiple Quantities}
While each of the measurements required to produce a roofline plot has it's own
subtleties, there are some effects affecting multiple measurements, which we
will discuss in the following sections.

\subsection{Context Switches}
Unless a special operating system is used, we have to deal with context switches
during measurement. On current operating systems, a context switch typically
occurs every 10ms due to the timer interrupt. Following the lines of
\cite{ComSysProgPersp}, there are two ways to deal with them:

If the execution time of the kernel is small, the operating system will
eventually execute the whole kernel without interruption. This can be exploited
using the best k measurement scheme. The kernel is repeatedly executed until the
k measurements with the smallest execution times show a variation below a
certain threshold. These executions were apparently not affected by a context
switch, since a context switch would have increased execution time. The
HW\_INT\_RCV performance counter could possibly be used as well to detect
context switches.

If a single execution of the kernel takes longer than the period of time between
two timer interrupts, it will always be affected by a context switch. In this
case, the proposed solution is to execute the kernel in a loop until many
context switches occurred. The effect of the context switches can be compensated
for by reducing the measured time be a certain factor. The factor depends on the
actual system.

The linux kernel offers the possibility to count performance events only during
the time a thread actually executes, omitting events happening during kernel
execution or while other threads run. This was not treated in the book above,
and offers interesting new options.



\subsection{System Load}
Specially if the measurement includes context switches, the system load has a
big impact on the measurement result. But due to caching effects, even the k
best measurement scheme might be affected by the system load. Since we can
control the system the measurements are performed on, we can control system
load. But none the less, it should be recorded along with the measurement, in
case something goes wrong with the measurement system setup.

\subsection{Multi Threading}
Our kernels and benchmarks will use multi threading. This has to be supported by
the measurement tool. Since some caches as well as part of the memory bandwidth
might be shared among different cores, multi threading can have various effects
on the amount of transferred memory and the available bandwidth.

In case of hyper threading, some functional units of a processor core are shared
among two threads. This influences the peak performance of the core.

It is possible that the operating system moves a thread from one core to
another. Since the overhead of a switching the core is large, it is generally
avoided by the scheduler of the operating system. But it can occur, and we have
to be prepared for it.

\section{Figures}
\listoffigures

\section{Bibliography}
\bibliographystyle{abbrv}
\bibliography{report}


\end{document}
