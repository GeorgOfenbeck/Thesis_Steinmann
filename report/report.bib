@techreport{Roofline,
    Author = {Williams, Samuel Webb and Waterman, Andrew and Patterson, David A.},
    Title = {Roofline: An Insightful Visual Performance Model for Floating-Point Programs and Multicore Architectures},
    Institution = {EECS Department, University of California, Berkeley},
    Year = {2008},
    Month = {Oct},
    URL = {http://www.eecs.berkeley.edu/Pubs/TechRpts/2008/EECS-2008-134.html},
    Number = {UCB/EECS-2008-134},
    Abstract = {We propose an easy-to-understand, visual performance model that offers insights to programmers and architects on improving parallel software and hardware for floating point computations.}
}

@book{ComSysProgPersp,
  title={Computer systems: a programmer's perspective},
  author={Bryant, R. and O'Hallaron, D.R.},
  isbn={9780136108047},
  lccn={2009053083},
  url={http://books.google.com/books?id=l0BfQgAACAAJ},
  year={2011},
  publisher={Prentice Hall}
}

@Article{siek01_concept_check,
author = {Jeremy G. Siek and Andrew Lumsdaine},
title = {{C++} Concept Checking},
journal = {Dr. Dobb's Journal},
year = 2001,
month = {June} 
}

@article{Whaley:2008:AAC:1462062.1462065,
 author = {Whaley, R. Clint and Castaldo, Anthony M.},
 title = {Achieving accurate and context-sensitive timing for code optimization},
 journal = {Softw. Pract. Exper.},
 issue_date = {December 2008},
 volume = {38},
 number = {15},
 month = dec,
 year = {2008},
 issn = {0038-0644},
 pages = {1621--1642},
 numpages = {22},
 url = {http://dx.doi.org/10.1002/spe.v38:15},
 doi = {10.1002/spe.v38:15},
 acmid = {1462065},
 publisher = {John Wiley \& Sons, Inc.},
 address = {New York, NY, USA},
 keywords = {ATLAS, benchmarking, cache flushing, kernel optimization, timers, timing},
} 

@book{evans2004domain,
  title={Domain-driven design: tackling complexity in the heart of software},
  author={Evans, E.},
  isbn={9780321125217},
  lccn={30503310},
  url={http://books.google.ch/books?id=7dlaMs0SECsC},
  year={2004},
  publisher={Addison-Wesley}
}

@TechReport{make,
author = {Richard M. Stallman, Roland McGrath, Paul D. Smith},
title = {GNU make Version 3.82},
institution = {Free Software Foundation},
year = 2010,
month = jul,
keyword = {make build recompilation},
}

@article{FreeLunchIsOver,
  added-at = {2008-09-03T09:42:11.000+0200},
  author = {Sutter, Herb},
  biburl = {http://www.bibsonomy.org/bibtex/2215f355123504f02d0ddb1b40292abeb/gron},
  interhash = {1540da30b9424bf778012a96f1192730},
  intrahash = {215f355123504f02d0ddb1b40292abeb},
  journal = {Dr. Dobb’s Journal},
  keywords = {Concurrency PhD Proposal},
  number = 3,
  pages = {202--210},
  timestamp = {2008-09-03T09:42:11.000+0200},
  title = {The Free Lunch Is Over: A Fundamental Turn Toward Concurrency in Software},
  url = {http://www.gotw.ca/publications/concurrency-ddj.htm},
  volume = 30,
  year = 2005
}

@misc{
unoffPerfEventsWebPage,
Author = {Vince Weaver},
Title = {The Unofficial Linux Perf Events Web-Page},
howpublished = {\url{http://web.eecs.utk.edu/~vweaver1/projects/perf-events/}} }

@misc{
libpfm4Docu,
Title = {Libpfm4 documentation},
howpublished = {\url{http://perfmon2.sourceforge.net/docs_v4.html}} }

@misc{
accuracyAndPrecision,
Author = {Wikipedia},
Title = {Accuracy and precision},
howpublished = {\url{http://en.wikipedia.org/wiki/Accuracy_and_precision}} }

@misc{
FastFourierTransform,
Author = {Wikipedia},
Title = {Fast Fourier transform},
howpublished = {\url{http://en.wikipedia.org/wiki/Fast_Fourier_transform}} }

@misc{
BoxPlot,
Author = {Wikipedia},
Title = {Box Plot},
howpublished = {\url{http://en.wikipedia.org/wiki/Box_plot}} }

@book{Press:2007:NRE:1403886,
 author = {Press, William H. and Teukolsky, Saul A. and Vetterling, William T. and Flannery, Brian P.},
 title = {Numerical Recipes 3rd Edition: The Art of Scientific Computing},
 year = {2007},
 isbn = {0521880688, 9780521880688},
 edition = {3},
 publisher = {Cambridge University Press},
 address = {New York, NY, USA},
}

@misc{
FFTW,
Title = {FFTW},
howpublished = {\url{http://www.fftw.org/}} }

@misc{
Spiral,
Title = {SPIRAL},
howpublished = {\url{http://www.spiral.net/}} }

@misc{
MKL,
Author ={Intel},
Title = {Intel Math Kernel Library},
howpublished = {\url{http://software.intel.com/en-us/articles/intel-mkl/}} }

@misc{
OpenBlas,
Title = {OpenBLAS},
howpublished = {\url{http://xianyi.github.com/OpenBLAS/}} }

@article{Hill:1989:EAC:76602.76603,
 author = {Hill, M. D. and Smith, A. J.},
 title = {Evaluating Associativity in CPU Caches},
 journal = {IEEE Trans. Comput.},
 issue_date = {December 1989},
 volume = {38},
 number = {12},
 month = dec,
 year = {1989},
 issn = {0018-9340},
 pages = {1612--1630},
 numpages = {19},
 url = {http://dx.doi.org/10.1109/12.40842},
 doi = {10.1109/12.40842},
 acmid = {76603},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
 keywords = {CPU caches, all-associativity simulation, associativity, buffer storage, cache miss ratio, content-addressable storage., direct-mapped, forest simulation, set-associative, stack simulation},
} 

@techreport{Asanovic:EECS-2006-183,
    Author = {Asanovic, Krste and Bodik, Ras and Catanzaro, Bryan Christopher and Gebis, Joseph James and Husbands, Parry and Keutzer, Kurt and Patterson, David A. and Plishker, William Lester and Shalf, John and Williams, Samuel Webb and Yelick, Katherine A.},
    Title = {The Landscape of Parallel Computing Research: A View from Berkeley},
    Institution = {EECS Department, University of California, Berkeley},
    Year = {2006},
    Month = {Dec},
    URL = {http://www.eecs.berkeley.edu/Pubs/TechRpts/2006/EECS-2006-183.html},
    Number = {UCB/EECS-2006-183},
    Abstract = {The recent switch to parallel microprocessors is a milestone in the history of computing. Industry has laid out a roadmap for multicore designs that preserves the programming paradigm of the past via binary compatibility and cache coherence. Conventional wisdom is now to double the number of cores on a chip with each silicon generation.

A multidisciplinary group of Berkeley researchers met nearly two years to discuss this change. Our view is that this evolutionary approach to parallel hardware and software may work from 2 or 8 processor systems, but is likely to face diminishing returns as 16 and 32 processor systems are realized, just as returns fell with greater instruction-level parallelism.

We believe that much can be learned by examining the success of parallelism at the extremes of the computing spectrum, namely embedded computing and high performance computing. This led us to frame the parallel landscape with seven questions, and to recommend the following:
<ul>
<li>The overarching goal should be to make it easy to write programs that execute efficiently on highly parallel computing systems
<li>The target should be 1000s of cores per chip, as these chips are built from processing elements that are the most efficient in MIPS (Million Instructions per Second) per watt, MIPS per area of silicon, and MIPS per development dollar.
<li>Instead of traditional benchmarks, use 13 "Dwarfs" to design and evaluate parallel programming models and architectures. (A dwarf is an algorithmic method that captures a pattern of computation and communication.)
<li>"Autotuners" should play a larger role than conventional compilers in translating parallel programs.
<li>To maximize programmer productivity, future programming models must be more human-centric than the conventional focus on hardware or applications. 
<li>To be successful, programming models should be independent of the number of processors.
<li>To maximize application efficiency, programming models should support a wide range of data types and successful models of parallelism: task-level parallelism, word-level parallelism, and bit-level parallelism.
<li>Architects should not include features that significantly affect performance or energy if programmers cannot accurately measure their impact via performance counters and energy counters.
<li>Traditional operating systems will be deconstructed and operating system functionality will be orchestrated using libraries and virtual machines.
<li>To explore the design space rapidly, use system emulators based on Field Programmable Gate Arrays (FPGAs) that are highly scalable and low cost.
</ul>

Since real world applications are naturally parallel and hardware is naturally parallel, what we need is a programming model, system software, and a supporting architecture that are naturally parallel. Researchers have the rare opportunity to re-invent these cornerstones of computing, provided they simplify the efficient programming of highly parallel systems.}
}

@INPROCEEDINGS{Boyd94ahierarchical,
    author = {Eric Boyd and Waqar Azeem and Hsien-Hsin Lee and Tien-Pao Shih and Shih-Hao Hung and Ed Davidson},
    title = {A Hierarchical Approach to Modeling and Improving the Performance of Scientific Applications on the KSR1},
    booktitle = {of Scientific Applications on the KSR1”, Proceedings of the 1994 International Conference on Parallel Processing},
    year = {1994},
    pages = {188--192}
}